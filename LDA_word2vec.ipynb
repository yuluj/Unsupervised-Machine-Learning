{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# BA 820 Homework 3 (100 Points)\n",
    "\n",
    "## 1 Latent Dirichlet Allocation [60pts]\n",
    "\n",
    "In this problem, we will use Latent Dirichlet Allocation to perform topic modeling on Amazon Review datasets. In particular, we will take an in-depth look at different aspects of LDA model.\n",
    "\n",
    "## 1.1 Installation\n",
    "\n",
    "To perform LDA and visualize, please use Python 3.X. You will also need to install Numpy, Scipy, gensim, nltk, pyLDAvis library. Refer to requirements.txt for more details.\n",
    "Use the following code to install the labraries."
   ],
   "metadata": {
    "id": "1ozalcZj6GnO"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K7m0zKTQLP-a"
   },
   "outputs": [],
   "source": [
    "%pip install gensim\n",
    "# install gensim for LDA\n",
    "%pip install nltk \n",
    "# install nltk to preprocess sentences\n",
    "%pip install pyldavis\n",
    "# to visualize LDA topics\n",
    "%pip install matplotlib \n",
    "# for plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "The cell below tests if the packages we need have been installed correctly, and that we are in the correct environment."
   ],
   "metadata": {
    "id": "QFDfBgMF6Rsu"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import gensim\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "import pyLDAvis\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import gzip # to unzip the data\n",
    "import re # to replace punctuations\n",
    "from nltk.corpus import stopwords # list of stopwords"
   ],
   "metadata": {
    "id": "9iRwy_ck6SLL"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.2 Datasets\n",
    "\n",
    "You can download the Amazon reviews dataset of Cellphones & Accessory 5-Core Data [here](http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Cell_Phones_and_Accessories_5.json.gz). Place the downloaded dataset in the same folder as this notebook. You can use the following code to read a datat from GZIp file"
   ],
   "metadata": {
    "id": "aLu3zAuv6m0r"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# A function to read the zipped data at a specfic path\n",
    "#\n",
    "# How to use:\n",
    "# PATH = \"/path/to/file\"\n",
    "# for line in parse(PATH):\n",
    "#   do something with line\n",
    "#\n",
    "def parse(path):\n",
    "    g = gzip.open(path, 'r')\n",
    "    for l in g:\n",
    "        yield eval(l)"
   ],
   "metadata": {
    "id": "gbSEVGKw6nV7"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.3 Data Cleaning\n",
    "\n",
    "Now we will preprocess the data using the following steps:\n",
    "   1. Remove stopwords\n",
    "   2. Lower-case all words\n",
    "   3. Remove words with less than 2 characters\n",
    "   4. Remove punctuation\n",
    "   5. Split each sentence into a list of words"
   ],
   "metadata": {
    "id": "B48OODRf70mY"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# A function to clean a single line of text\n",
    "def clean_line(line):\n",
    "    \"\"\" Clean stopwords and punction for each line\n",
    "    \n",
    "    Args: \n",
    "        line (string): one line in file\n",
    "        \n",
    "    Returns:\n",
    "        list(str): a list of all words in the sentence\n",
    "    \"\"\"\n",
    "    punctuationRegex = r'\\W+|\\d+'\n",
    "    stopWords = set(stopwords.words('english'))\n",
    "    line = line.split(\" \")\n",
    "    filtered_content = []\n",
    "    for word in line:\n",
    "\n",
    "        #########################\n",
    "        # YOUR CLEANING CODE HERE\n",
    "        #########################\n",
    "    return filtered_content"
   ],
   "metadata": {
    "id": "6JVvT5kh7qO9"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, we put parse() and clean_line() function together and then extract the first 10,000 reviews into a new text file as your experiment dataset"
   ],
   "metadata": {
    "id": "fYnnn1KC8jZ8"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def read_dataset(fname):\n",
    "    \"\"\" Read the 10000 lines in given dataset into list and clean stop words. \n",
    "        \n",
    "    Args: \n",
    "        fname (string): filename of Amazon Review Dataset\n",
    "        \n",
    "    Returns:\n",
    "        list of list of words: we view each document as a list, including a list of all words \n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    exp_dataset = []\n",
    "    for review in parse(fname):\n",
    "        line = review[\"reviewText\"]\n",
    "        new_line = clean_line(line)\n",
    "        exp_dataset.append(new_line)\n",
    "        count += 1\n",
    "        if count > 10000:\n",
    "            break\n",
    "    return exp_dataset"
   ],
   "metadata": {
    "id": "FXjJSwnD8jvK"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "%%time\n",
    "r = read_dataset(\"reviews_Cell_Phones_and_Accessories_5.json.gz\")"
   ],
   "metadata": {
    "id": "4cRY1JaN8mIM"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.4 Topic Analysis\n",
    "\n",
    "**[5pts] Q1.4.1.1** Use topic numbers 3, 6, 9, 12, 15 respectively and print out all topics with 5 words.\n",
    "\n",
    "For this We will use gensim to train an LDA model. gensim requires the following steps:\n",
    "\n",
    "Construct a gensim.corpora.dictionary from the dataset\n",
    "Construct a gensim \"corpus\" using this dictionary, by mapping each word to an index in the dictionary\n",
    "Run LDA on this corpus"
   ],
   "metadata": {
    "id": "2FQFnMjb8l1B"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "dictionary = None # create a gensim dictionary, store it in variable \"dictionary\"\n",
    "corpus = None # create the gensim corpus, store it in variable \"corpus\""
   ],
   "metadata": {
    "id": "JzfOjF9I9kDD"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "The function below prints the top num words in each topic for a given model."
   ],
   "metadata": {
    "id": "xblcs_6-99gp"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def print_topic_words(model,num):\n",
    "    \"\"\" print top words in model topics.\n",
    "    \n",
    "    Args: \n",
    "        model: LDA model\n",
    "        \n",
    "    Returns:\n",
    "        none\n",
    "    \"\"\"    \n",
    "    #########################\n",
    "    # YOUR CODE HERE\n",
    "    #########################\n",
    "    return"
   ],
   "metadata": {
    "id": "b_NopMJj951v"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following function builds multiple LDA models with number of topics specified in the list `num_topics`."
   ],
   "metadata": {
    "id": "56Sv1l9S-JeI"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def build_num_topic_model(dictionary, corpus, num_topics):\n",
    "    \"\"\" Build lda model with given parameters, use print_topic_words to print words\n",
    "    \n",
    "    Args: \n",
    "        dictionary: dictionary built from dataset\n",
    "        corpus: corpus built from dataset\n",
    "        num_topics: list of numbers\n",
    "        \n",
    "    Returns:\n",
    "        none\n",
    "    \"\"\"    \n",
    "    for num_topic in num_topics:\n",
    "        #########################\n",
    "        # YOUR CODE HERE\n",
    "        #   - Build model\n",
    "        #   - Print the top 5 words\n",
    "        #########################"
   ],
   "metadata": {
    "id": "NhGPQYTo-Jxx"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "build_num_topic_model(dictionary, corpus, [3, 6, 9, 12, 15])"
   ],
   "metadata": {
    "id": "k8NtFThx-Od0"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**[3pts] Q1.4.1.2**  Explain what could be interpreted for each topics, and describe the similarity and difference between different topic numbers."
   ],
   "metadata": {
    "id": "m8vkk6bo-UEi"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "-write your answer here"
   ],
   "metadata": {
    "id": "BRLNs647-VF7"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**[2pts] Q1.4.1.3**  Which topic number would you choose? Explain."
   ],
   "metadata": {
    "id": "ERTTf2sGDIPp"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "-write your answer here"
   ],
   "metadata": {
    "id": "UOgDUPcvDLs8"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.5 Model Evaluation\n",
    "\n",
    "**[12 pts] Q1.5.1** Now we investigate two methods to evaluate our model and choose the topic number\n",
    "\n",
    "1.Perplexity is a measurement of how well a probability distribution or probability model predicts a sample. A low perplexity indicates the probability distribution is good at predicting the sample. We can use model.log_perplexity(document) to evaluate the perplexity of our LDA model.\n",
    "\n",
    "2.Topic coherence is a one type of interpretability measurement for a topic. It measures if a set of top keywords describe a coherent and singular concept. A good topic will have high topic coherence score. We can use CoherenceModel(model=ldamodel).get_coherence() to calculate it.\n",
    "\n",
    "Plot Perplexity and topic coherence scores of our LDA model for topic number 3,6,9,12,15,20,50."
   ],
   "metadata": {
    "id": "sJfqY1reDX5s"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The code below trains topic models with different numbers of topics and measures their coherence and perplexity."
   ],
   "metadata": {
    "id": "ni7eZRSIDsIU"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# perplexity \n",
    "# run different number of topics to get perplexity and coherence value for this model\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "def get_measurement_for_model(dictionary, corpus, topic_nums):\n",
    "    \"\"\" Build lda model with given parameters \n",
    "    \n",
    "    Args: \n",
    "        dictionary: dictionary built from dataset\n",
    "        corpus: corpus built from dataset\n",
    "        topic_nums: a list contains all possible topic number\n",
    "        \n",
    "    Returns:\n",
    "        2 lists: one of perplexities, and one of coherence value\n",
    "    \"\"\"  \n",
    "    perplexity = []\n",
    "    coherence_value=[]\n",
    "    for num_topic in topic_nums:\n",
    "        #########################\n",
    "        # YOUR CODE HERE\n",
    "        #   - Build model\n",
    "        #   - Compute and store coherence\n",
    "        #   - Compute and store perplexity\n",
    "        #########################\n",
    "    return perplexity,coherence_value"
   ],
   "metadata": {
    "id": "1ue1Pf4--UjO"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "perplexity, coherence = get_measurement_for_model(dictionary, corpus, [3, 6, 9, 12, 15, 20, 50])"
   ],
   "metadata": {
    "id": "nX_zj0RoD4j0"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(perplexity)\n",
    "print(coherence)"
   ],
   "metadata": {
    "id": "yk-ojlJSD85k"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can now plot the coherence and perplexity of each model."
   ],
   "metadata": {
    "id": "PcYTqBizD9az"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt"
   ],
   "metadata": {
    "id": "1B9scv8sD_n8"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "plt.plot([3, 6, 9, 12, 15, 20, 50], perplexity)\n",
    "plt.grid()\n",
    "plt.xlabel(\"Number of topics\")\n",
    "plt.ylabel(\"Perplexity\")\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "G76jh0BXECKs"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**[2pts] Q1.5.2**  From the above graph what topic number would you choose and why? Is it a good idea to choose the topic number based on perplexity? why or why not?"
   ],
   "metadata": {
    "id": "WUDG-rr5ELQN"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "-write your answer here"
   ],
   "metadata": {
    "id": "q5SLdipiENoi"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "plt.plot([3, 6, 9, 12, 15, 20, 50], coherence)\n",
    "plt.grid()\n",
    "plt.xlabel(\"Number of topics\")\n",
    "plt.ylabel(\"Coherence\")\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "EBhTyUQdELuW"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**[2pts] Q1.5.3**  From the above graph what topic number would you choose and why?"
   ],
   "metadata": {
    "id": "VnCOd2_CEY2W"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "-Write your answer here"
   ],
   "metadata": {
    "id": "Mm5bcHV4EcOa"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**[4pts]Q1.5.4** Compare two methods you implemented in the previous quesions, which one do you think is better and why? In answering, please discuss the actual topics generated."
   ],
   "metadata": {
    "id": "ppxl4L4cElVA"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "-Write your answer here"
   ],
   "metadata": {
    "id": "XMiQEut8Em__"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.6 Alpha and Beta in LDA"
   ],
   "metadata": {
    "id": "hBkDhQzGFPyp"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**[7pts]Q1.6.1** In this problem, we will check the two most important parameters in LDA model: alpha and beta. Alpha represents document-topic density - with a higher alpha, documents are made up of more topics, and with lower alpha, documents contain fewer topics. Beta represents topic-word density - with a high beta, topics are made up of most of the words in the corpus, and with a low beta they consist of few words."
   ],
   "metadata": {
    "id": "Jelhz4KFFgde"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "best_topic_num = YOUR_VALUE HERE # CHANGE THIS"
   ],
   "metadata": {
    "id": "x7Wr3DphEb2O"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#model 1\n",
    "model1 = None\n",
    "#########################\n",
    "# YOUR CODE HERE\n",
    "#   - Build model for alpha = 1/num_topic = eta\n",
    "#   - Print top words\n",
    "#########################"
   ],
   "metadata": {
    "id": "8AP4Hn1uFkkc"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#model 2\n",
    "model2 = None\n",
    "#########################\n",
    "# YOUR CODE HERE\n",
    "#   - Build model for alpha = 1/2, eta = 1/5\n",
    "#   - Print top words\n",
    "#########################"
   ],
   "metadata": {
    "id": "3WDzHIzGFm7v"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#model 3\n",
    "model3 = None\n",
    "#########################\n",
    "# YOUR CODE HERE\n",
    "#   - Build model for alpha = 'auto' = eta\n",
    "#   - Print top words\n",
    "#########################"
   ],
   "metadata": {
    "id": "hhM9efkXFqI0"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**[3pts]1.6.2**  Explain how the different alpha and beta values theoretically influence the LDA model. Then describe what you find in the empirical result (e.g difference in topic words and topics)"
   ],
   "metadata": {
    "id": "WRIg6fAIFwW0"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "-Write your answer here"
   ],
   "metadata": {
    "id": "QU26drlOFxo1"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.7 LDA on a short text dataset"
   ],
   "metadata": {
    "id": "lSsPpXqXG11h"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**[10pts]1.7.1** In this part, we will read a dataset from twitter and build a LDA model. On Windows, download and unzip the dataset from [this link](http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip). Place the downloaded dataset in the same folder as this notebook. Use the first 10,000 lines in the \"training.1600000.processed.noemoticon.csv\" file. "
   ],
   "metadata": {
    "id": "pbrCxteDHM1b"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!wget http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip # Linux and OSX only\n",
    "!unzip trainingandtestdata.zip # Linux and OSX only"
   ],
   "metadata": {
    "id": "Eg8B4xaRFxA3"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!head -n 10000 training.1600000.processed.noemoticon.csv > twitter.csv # Linux and OSX only"
   ],
   "metadata": {
    "id": "81FoZvt_HTq6"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def read_twitter(fname):\n",
    "    \"\"\" Read the given dataset into list and clean stop words. \n",
    "    \n",
    "    Args: \n",
    "        fname (string): filename of Twitter Dataset\n",
    "        \n",
    "    Returns:\n",
    "        list of list of words: we view each document as a list, including a list of all words \n",
    "    \"\"\"\n",
    "    twitter = []\n",
    "    with open(fname,encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            tweet = f.readline().split(\",\")[5]\n",
    "            ########################\n",
    "            # YOUR CLEANING CODE HERE\n",
    "            #    - Clean tweet\n",
    "            #    - Split into list words\n",
    "            #    - Store list in twitter\n",
    "            ########################\n",
    "    return twitter"
   ],
   "metadata": {
    "id": "uJSwYJflHWvp"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "%%time\n",
    "twitter = read_twitter('twitter.csv')"
   ],
   "metadata": {
    "id": "1657cE1hHY8_"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "twitter_dictionary = None # TODO: build dictionary\n",
    "twitter_corpus = None # TODO: build corpus for model"
   ],
   "metadata": {
    "id": "YzcgFZx_HZgK"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "%%time\n",
    "########################\n",
    "# YOUR CODE HERE\n",
    "#    - Build model\n",
    "#    - Print top words\n",
    "########################"
   ],
   "metadata": {
    "id": "3WoNVLuJHcyu"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.8 LDA visualization\n",
    "\n",
    "**[10pts]1.8.1** We will now visualize the LDA output using pyLDAvis. PyLDAVis shows the following:\n",
    "\n",
    "   1. The distances between topics, as a map in 2-D space.\n",
    "   2. The variance in the topic-word distribution, as the size of a circle in this map.\n",
    "   3. The most \"salient\" terms in each topic."
   ],
   "metadata": {
    "id": "lzced6IXOU1Z"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "sentences = read_dataset(\"reviews_Cell_Phones_and_Accessories_5.json.gz\")[:1000] # CHANGE TO YOUR DATASET"
   ],
   "metadata": {
    "id": "iaSJ2dyHOVlM"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "%%time\n",
    "import pyLDAvis.gensim"
   ],
   "metadata": {
    "id": "5Dn0msKKPJXF"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "%%time\n",
    "data = None\n",
    "########################\n",
    "# YOUR CODE HERE\n",
    "#   - Initalize pyLDAvis with your model\n",
    "#   - Make sure to use a subset of the sentences in the dataset\n",
    "#     if your pyLDAvis call in the cell below is taking too long\n",
    "########################"
   ],
   "metadata": {
    "id": "uZsFH15wPLo2"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "pyLDAvis.display(data)"
   ],
   "metadata": {
    "id": "D_BnRDcsPNyX"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2.word2Vec [40pts]\n",
    "\n",
    "\n",
    "In this problem, we use Amazon Review Dataset to perform Word2Vec and Doc2Vec to extract insights relevant for e-commerce business. For this question, download and use the dataset [here](http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Electronics_5.json.gz||reviews_Electronics_5.json.gz)."
   ],
   "metadata": {
    "id": "Ukn2e3iWRoIz"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.1 Data Cleaning\n",
    "\n"
   ],
   "metadata": {
    "id": "-UKcbiFCRuJr"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following code reads the data from a GZIP file."
   ],
   "metadata": {
    "id": "mcw3Abp6R549"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# A function to read the zipped data at a specfic path\n",
    "#\n",
    "# How to use:\n",
    "# PATH = \"/path/to/file\"\n",
    "# for line in parse(PATH):\n",
    "#   do something with line\n",
    "#\n",
    "def parse(path):\n",
    "    g = gzip.open(path, 'r')\n",
    "    for l in g:\n",
    "        yield eval(l)"
   ],
   "metadata": {
    "id": "T-HJ5AnnRpFB"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will now read the data and preprocess it using the following steps:\n",
    "\n",
    "   1. Remove stopwords\n",
    "   2. Lower-case all words\n",
    "   3. Remove words with less than 2 characters\n",
    "   4. Remove punctuation\n",
    "   5. Split each sentence into a list of words\n",
    "\n",
    "   And finally extract 10000 reviews."
   ],
   "metadata": {
    "id": "O543cF24R81Z"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# A function to clean a single line of text\n",
    "def clean_line(line):\n",
    "    \"\"\" Clean stopwords and punction for each line\n",
    "    \n",
    "    Args: \n",
    "        line (string): one line in file\n",
    "        \n",
    "    Returns:\n",
    "        list(str): a list of all words in the sentence\n",
    "    \"\"\"\n",
    "    line = line.split(\" \")\n",
    "    filtered_content = []\n",
    "    for word in line:\n",
    "        #########################\n",
    "        # YOUR CLEANING CODE HERE\n",
    "        #########################\n",
    "    return filtered_content\n",
    "\n",
    "def read_dataset(fname):\n",
    "    \"\"\" Read the 100000 lines in given dataset into list and clean stop words. \n",
    "        \n",
    "    Args: \n",
    "        fname (string): filename of Amazon Review Dataset\n",
    "        \n",
    "    Returns:\n",
    "        list of list of words: we view each document as a list, including a list of all words \n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    exp_dataset = []\n",
    "    for review in parse(fname):\n",
    "        line = review[\"reviewText\"]\n",
    "        new_line = clean_line(line)\n",
    "        exp_dataset.append(new_line)\n",
    "        count += 1\n",
    "        if count > 100000:\n",
    "            break\n",
    "    return exp_dataset    "
   ],
   "metadata": {
    "id": "4_GNZ3ikR_rE"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "%%time\n",
    "r = read_dataset(\"reviews_Electronics_5.json.gz\")"
   ],
   "metadata": {
    "id": "zaf3YFgHTNr3"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.2 Build a doc2vec model"
   ],
   "metadata": {
    "id": "JamafEPNS3Ii"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**[3pts]2.2.1** In this question, first we will build a Word2Vec model using ginsim using size=300, min_count=40, win- dow=10, negative=10, max_vocab_size=10000. Train the model for 30 epochs."
   ],
   "metadata": {
    "id": "BFMVOQgrTi9s"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from gensim.models import Word2Vec\n",
    "# YOUR CODE HERE"
   ],
   "metadata": {
    "id": "5gGK2QnKTfdO"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**[2pts]2.2.2** Use model.wv.doesnt_match to find a word in [\"Canon\",\"Nikon\",\"junk\"] that does not\n",
    "\n",
    "belong."
   ],
   "metadata": {
    "id": "IPnJE0pQT6jr"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# YOUR CODE HERE [\"Canon\", \"Nikon\", \"junk\"]"
   ],
   "metadata": {
    "id": "pGQT-f_OTqnX"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**[3pts]2.2.3** Come up with 3 other word lists and apply the above function. Explain your observation."
   ],
   "metadata": {
    "id": "a44R5v_-UFXp"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# write your code here"
   ],
   "metadata": {
    "id": "GP9dTi1yUNGR"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "-Write your answer here"
   ],
   "metadata": {
    "id": "mlZKiD_DUN00"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**[2pts]2.2.4** What are some tasks in e-commerce that can be solved with this simple function?"
   ],
   "metadata": {
    "id": "SLz_WXiHUG4L"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "-Write your answer here"
   ],
   "metadata": {
    "id": "IjaKCXMEUYyM"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.3 Build a doc2vec model"
   ],
   "metadata": {
    "id": "2ZmhD59iUtR3"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**[15 pts] 2.3.1**  Each review is marked by other customers as “helpful” or not. The \"helpful: [a, b]\" item in each review is (a) the number of people who marked the review as helpful, and (b) the total number of people who have marked the review as helpful or unhelpful. The \"helpfulness\" score of a review can be calculated as a/b. Define a \"helpful\" review as one with helpfulness score >= 0.8. Given a review that is only slightly helpful, could we find textually similar reviews but have higher helpfulness? Build Doc2Vec model with gensim on review data. Use product ID “B00006I5WJ” and ReviewerID with “A14453U0KFWF31” as an example, find top 5 helpful reviews of the same product with similarity score above 0.8. "
   ],
   "metadata": {
    "id": "cRC20sS1VYJN"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "def read_reviewers_data(fname, min_count=0):\n",
    "    '''\n",
    "    Save all reviews into their own product asin files.\n",
    "    Make sure you have 'product' folder when you run this answer.\n",
    "    In each file, you can choose your own log structure. In this answer, log strucutre is like \n",
    "        \"reviewText\"\\t\"reviewerID\"\\t\"helpful\"\n",
    "    Args: \n",
    "        fname: dataset file path\n",
    "        min_count: minimum number of reviews of a product\n",
    "    Returns:\n",
    "        none\n",
    "    '''\n",
    "    if not os.path.isdir('product'):\n",
    "        os.makedirs('product')\n",
    "    asin_list = []\n",
    "    tmp_list = []\n",
    "    last_asin = \"\"\n",
    "    j = 0\n",
    "    for i in parse(fname):\n",
    "        if last_asin != i['asin']:\n",
    "            if len(tmp_list) > min_count:\n",
    "                f = open(\"product/\" + last_asin+\".txt\", 'w')\n",
    "                for one in tmp_list:\n",
    "                    f.write(one)\n",
    "                f.close()\n",
    "            tmp_list = []\n",
    "            last_asin = i['asin']\n",
    "        tmp_list.append(i[\"reviewText\"] + '\\t' + i[\"reviewerID\"] +\n",
    "                    '\\t' + handle_helpful(i[\"helpful\"]) + \"\\n\")\n",
    "        j += 1\n",
    "        if j > 100000:\n",
    "            break\n",
    "            \n",
    "def handle_helpful(helpful):\n",
    "    '''\n",
    "    Helper function for helpful_score calculate\n",
    "    Args: \n",
    "        helpful: list. The first element is the number of people think this is helpful. The second element\n",
    "            is the total number of people evaluate this comment\n",
    "    Returns:\n",
    "        String: number represent helpfulness\n",
    "    '''\n",
    "    if helpful[1] != 0:\n",
    "        helpfulness = 1.0 * helpful[0] / helpful[1]\n",
    "        return str(helpfulness)\n",
    "    else:\n",
    "        return str(0)"
   ],
   "metadata": {
    "id": "xGDDL9a4UBGi"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "read_reviewers_data(\" reviews_Electronics_5.json.gz\")"
   ],
   "metadata": {
    "id": "ZcaWNGSyban3"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class TaggedReviewDocument(object):\n",
    "    '''\n",
    "    This class could save all products and review information in its dictionary and generate iter for TaggedDocument\n",
    "        which could used for Doc2Vec model\n",
    "    '''\n",
    "    def __init__(self, dirname):\n",
    "        self.dirname = dirname\n",
    "        self.helpfulness = {}  # key:reviewerID value:helpfulness\n",
    "        self.product = {}      # key:asin value:reviewerID\n",
    "        self.asin = []\n",
    "\n",
    "    def __iter__(self):\n",
    "        for filename in os.listdir(self.dirname):\n",
    "            asin_code = filename[:-4] #delete \".txt\"\n",
    "            self.product[asin_code] = []\n",
    "            self.asin.append(asin_code)\n",
    "            for line in enumerate(open(self.dirname + \"/\" + filename)):\n",
    "                line_content = line[1].split(\"\\t\")\n",
    "                self.product[asin_code].append(line_content[1])\n",
    "                self.helpfulness[line_content[1]] = float(line_content[2])\n",
    "                yield TaggedDocument(clean_line(line_content[0]), [line_content[1], line_content[2]])\n"
   ],
   "metadata": {
    "id": "phyr9d9Zbvbe"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "documents = TaggedReviewDocument(\"product\")"
   ],
   "metadata": {
    "id": "59Ccalf7bwE2"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "%%time\n",
    "from gensim.models.doc2vec import TaggedDocument, Doc2Vec\n",
    "# YOUR CODE HERE"
   ],
   "metadata": {
    "id": "crSL0O9HbyPI"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Find similar reviews"
   ],
   "metadata": {
    "id": "HbabO2WUcZxJ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def find_similar_reviews(asin,reviewer_id):\n",
    "    '''\n",
    "    If one review is similar to the specefic review and it is much helpful, save it to a list\n",
    "    Args: \n",
    "        asin: product asin\n",
    "        reviewer_id: the specific review\n",
    "    Returns:\n",
    "        list of reviewer id\n",
    "    '''\n",
    "    result = []\n",
    "    ########################\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    ########################\n",
    "    return result"
   ],
   "metadata": {
    "id": "yCI8l3_9b0ru"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(find_similar_reviews(\"B00006I5WJ\", \"A14453U0KFWF31\")) "
   ],
   "metadata": {
    "id": "TU-cVz3cc6gO"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.4 Build a doc2vec model using product descriptions\n"
   ],
   "metadata": {
    "id": "6KIVR0OodJbc"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**[10pts]2.4.1** Use product descriptions (located in meta data  [here](http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/meta_Electronics.json.gz)) to build a Doc2Vec model. When building the doc2vec model, use vector_size=100, window=15, min_count=5, max_vocab_size=1000, and train it for 1 epoch."
   ],
   "metadata": {
    "id": "6aP5_XP5dO2i"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def read_product_description(fname):\n",
    "    '''\n",
    "    Load all product descriptions\n",
    "    Args: \n",
    "        fname: dataset file path\n",
    "    Returns:\n",
    "        dict: key is asin, value is description content\n",
    "    '''\n",
    "    result = {}\n",
    "    for i in parse(fname):\n",
    "        try:\n",
    "            if \"Camera & Photo\" in i[\"categories\"][0]:\n",
    "                result[i[\"asin\"]]=i[\"description\"]\n",
    "        except:\n",
    "            continue\n",
    "    return result"
   ],
   "metadata": {
    "id": "6j9r0LvidJ7V"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class TaggedDescriptionDocument(object):\n",
    "    '''\n",
    "    This class could save all products and review information in its dictionary and generate iter for TaggedDocument\n",
    "        which could used for Doc2Vec model\n",
    "    '''\n",
    "    def __init__(self, descriptondict):\n",
    "        self.descriptondict = descriptondict\n",
    "        \n",
    "\n",
    "    def __iter__(self):\n",
    "        for asin in self.descriptondict:\n",
    "            for content in self.descriptondict[asin]:\n",
    "                yield TaggedDocument(clean_line(content), [asin])\n"
   ],
   "metadata": {
    "id": "XrSISgv3df6_"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "description_dict = read_product_description(\"meta_Electronics.json.gz\")\n",
    "des_documents = TaggedDescriptionDocument(description_dict)"
   ],
   "metadata": {
    "id": "yTRwOCtldinm"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Build a doc2vec model\n",
    "# YOUR CODE HERE"
   ],
   "metadata": {
    "id": "0eG6BC6oeNUA"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**[5pts]2.4.2** Find the most similar product for Canon EOS 5D (asin:B0007Y791C) not made by Canon."
   ],
   "metadata": {
    "id": "pvwRTr6ueRS4"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#Write your code here"
   ],
   "metadata": {
    "id": "wrQZo-AleN-O"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "Ild0RccPeVU3"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
